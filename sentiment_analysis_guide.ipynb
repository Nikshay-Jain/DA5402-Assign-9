{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Amazon Reviews/Divvy Trips Data\n",
    "\n",
    "This notebook guides you through the process of running sentiment analysis on either:\n",
    "1. Amazon reviews dataset (as specified in the assignment)\n",
    "2. Adapted Divvy Trips data (in case you need to use the mentioned Divvy_Trips_2020_Q1 file)\n",
    "\n",
    "Let's start by setting up our environment and exploring the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch scikit-learn matplotlib seaborn nltk pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "### Option 1: If you're using the Amazon reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AmazonReviewsSentimentAnalysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the path to your Amazon reviews dataset\n",
    "amazon_data_path = \"path/to/amazon_reviews.csv\"\n",
    "\n",
    "# Load data\n",
    "amazon_df = spark.read.csv(amazon_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the schema\n",
    "amazon_df.printSchema()\n",
    "\n",
    "# Show a sample of the data\n",
    "amazon_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: If you're using the Divvy Trips dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run the adapter script to convert Divvy Trips data\n",
    "!python divvy_adapter.py --input \"path/to/Divvy_Trips_2020_Q1.csv\" --output \"adapted_divvy_data.csv\" --sample 10000\n",
    "\n",
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DivvyTripsSentimentAnalysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the adapted Divvy data\n",
    "divvy_df = spark.read.csv(\"adapted_divvy_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the schema\n",
    "divvy_df.printSchema()\n",
    "\n",
    "# Show a sample of the data\n",
    "divvy_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Task 1: Sentiment Analysis\n",
    "\n",
    "Let's apply sentiment analysis to the review texts using the pretrained Hugging Face pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from transformers import pipeline\n",
    "\n",
    "# Choose the appropriate dataframe based on your dataset\n",
    "# df = amazon_df  # Uncomment if using Amazon reviews\n",
    "df = divvy_df    # Uncomment if using Divvy trips\n",
    "\n",
    "# Initialize the sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Define UDF for sentiment analysis\n",
    "@udf(StringType())\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        if text is None or len(str(text).strip()) == 0:\n",
    "            return \"NEUTRAL\"\n",
    "        result = sentiment_analyzer(str(text))\n",
    "        return \"POSITIVE\" if result[0]['label'] == 'POSITIVE' else \"NEGATIVE\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment for text: {e}\")\n",
    "        return \"NEUTRAL\"\n",
    "\n",
    "# Apply sentiment analysis to the review text\n",
    "result_df = df.withColumn(\"predicted_sentiment\", analyze_sentiment(col(\"reviewText\")))\n",
    "\n",
    "# Show results\n",
    "result_df.select(\"reviewText\", \"predicted_sentiment\").show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Task 2: Model Evaluation\n",
    "\n",
    "Let's evaluate the sentiment analysis model by comparing predicted sentiment with true sentiment derived from ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import when\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert ratings to true sentiment labels (POSITIVE if rating >= 3.0, else NEGATIVE)\n",
    "df_with_true = result_df.withColumn(\n",
    "    \"true_sentiment\", \n",
    "    when(col(\"overall\") >= 3.0, \"POSITIVE\").otherwise(\"NEGATIVE\")\n",
    ")\n",
    "\n",
    "# Convert to Pandas for easier analysis\n",
    "pd_df = df_with_true.select(\"predicted_sentiment\", \"true_sentiment\").toPandas()\n",
    "\n",
    "# Filter out any NEUTRAL predictions for evaluation purposes\n",
    "pd_df = pd_df[pd_df['predicted_sentiment'] != \"NEUTRAL\"]\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(\n",
    "    pd_df['true_sentiment'], \n",
    "    pd_df['predicted_sentiment'],\n",
    "    labels=[\"POSITIVE\", \"NEGATIVE\"]\n",
    ")\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(\n",
    "    pd_df['true_sentiment'], \n",
    "    pd_df['predicted_sentiment'],\n",
    "    pos_label=\"POSITIVE\"\n",
    ")\n",
    "\n",
    "recall = recall_score(\n",
    "    pd_df['true_sentiment'], \n",
    "    pd_df['predicted_sentiment'],\n",
    "    pos_label=\"POSITIVE\"\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=[\"Predicted POSITIVE\", \"Predicted NEGATIVE\"],\n",
    "    yticklabels=[\"True POSITIVE\", \"True NEGATIVE\"]\n",
    ")\n",
    "plt.title('Sentiment Analysis Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Additional Analysis: Distribution of Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get full pandas DataFrame for visualization\n",
    "full_pd_df = df_with_true.toPandas()\n",
    "\n",
    "# Plot the distribution of true vs predicted sentiment\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(data=full_pd_df, x='true_sentiment')\n",
    "plt.title('Distribution of True Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=full_pd_df, x='predicted_sentiment')\n",
    "plt.title('Distribution of Predicted Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the Complete Pipeline\n",
    "\n",
    "Let's run the full pipeline using the main script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# For Amazon reviews dataset\n",
    "# !python main.py --input \"path/to/amazon_reviews.csv\" --output \"output\"\n",
    "\n",
    "# For adapted Divvy trips dataset\n",
    "!python main.py --input \"adapted_divvy_data.csv\" --output \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Examine the Results\n",
    "\n",
    "Let's look at the outputs generated by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display confusion matrix\n",
    "confusion_df = pd.read_csv(\"output/confusion_matrix.csv\", index_col=0)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display metrics\n",
    "with open(\"output/metrics.txt\", \"r\") as f:\n",
    "    metrics = f.read()\n",
    "print(\"Metrics:\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we've completed both tasks from the assignment:\n",
    "\n",
    "1. We used a pretrained sentiment analysis pipeline to classify review texts as POSITIVE or NEGATIVE\n",
    "2. We evaluated the model by comparing the predicted sentiment with true sentiment labels derived from ratings\n",
    "\n",
    "The solution follows the map-reduce paradigm for distributed processing, with proper error handling and logging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
